services:
  spark-master:
    image: our-own-apache-spark
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
       - ./jobs:/opt/spark-apps
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
    env_file:
      - ../config.env
    networks:
      - project-net

  spark-worker-a:
    image: our-own-apache-spark
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    env_file:
      - ../config.env
    volumes:
       - ./jobs:/opt/spark-apps
    networks:
      - project-net

  spark-worker-b:
    image: our-own-apache-spark
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "9092:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    env_file:
      - ../config.env
    volumes:
        - ./jobs:/opt/spark-apps
    networks:
      - project-net

  jupyter:
    image: jupyter/pyspark-notebook:spark-3.4.0
    ports:
      - "8888:8888"
    networks:
      - project-net
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    volumes:
      - ./notebooks:/home/jovyan/work
    env_file:
      - ../config.env
    environment:
      # Aponta para o master remoto
      - SPARK_MASTER=spark://spark-master:7077

      # JARs de S3A e Delta via --packages
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,io.delta:delta-core_2.12:2.4.0 pyspark-shell

      # Credenciais do MinIO (usadas pelo Hadoop S3A)
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}


networks:
  project-net:
    external: true
